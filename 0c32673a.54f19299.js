(window.webpackJsonp=window.webpackJsonp||[]).push([[5],{137:function(e,t,a){"use strict";a.d(t,"a",(function(){return b})),a.d(t,"b",(function(){return m}));var r=a(0),n=a.n(r);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function s(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?s(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var c=n.a.createContext({}),p=function(e){var t=n.a.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},b=function(e){var t=p(e.components);return n.a.createElement(c.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.a.createElement(n.a.Fragment,{},t)}},d=n.a.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),b=p(a),d=r,m=b["".concat(s,".").concat(d)]||b[d]||u[d]||i;return a?n.a.createElement(m,o(o({ref:t},c),{},{components:a})):n.a.createElement(m,o({ref:t},c))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,s=new Array(i);s[0]=d;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o.mdxType="string"==typeof e?e:r,s[1]=o;for(var c=2;c<i;c++)s[c]=a[c];return n.a.createElement.apply(null,s)}return n.a.createElement.apply(null,a)}d.displayName="MDXCreateElement"},195:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/data_migration-c76a3d388d6b659323c8a3e538a25a64.png"},196:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/data2-1ed1d95ace54a12b07be3b95f2f7facd.png"},197:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/data3-a93122c4b2c0d3e95b05b83e435f8963.png"},198:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/data4-da7082192e5e80119a19ddec3adf760a.png"},65:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return s})),a.d(t,"metadata",(function(){return o})),a.d(t,"toc",(function(){return l})),a.d(t,"default",(function(){return p}));var r=a(3),n=a(7),i=(a(0),a(137)),s={id:"data_migration",title:"Setting Up AWS DMS"},o={unversionedId:"data_migration",id:"data_migration",isDocsHomePage:!1,title:"Setting Up AWS DMS",description:"What is AWS DMS?",source:"@site/docs/data_migration.md",slug:"/data_migration",permalink:"/Infrastructure-Playbook/data_migration",editUrl:"https://github.com/LBHackney-IT/Infrastructure-Playbook/edit/master/docs/data_migration.md",version:"current"},l=[{value:"DMS supported replication types",id:"dms-supported-replication-types",children:[]},{value:"For continuous migration",id:"for-continuous-migration",children:[{value:"CDC",id:"cdc",children:[]},{value:"MS Replication",id:"ms-replication",children:[]}]},{value:"For one-off set up",id:"for-one-off-set-up",children:[{value:"Use cases",id:"use-cases-1",children:[]}]},{value:"Database set up",id:"database-set-up",children:[]},{value:"AWS DMS set up via Terraform",id:"aws-dms-set-up-via-terraform",children:[{value:"DMS",id:"dms",children:[]},{value:"Postgres",id:"postgres",children:[]}]},{value:"Data migration using a data pipeline",id:"data-migration-using-a-data-pipeline",children:[{value:"Data pipeline - CSV to Postgres",id:"data-pipeline---csv-to-postgres",children:[]}]}],c={toc:l};function p(e){var t=e.components,s=Object(n.a)(e,["components"]);return Object(i.b)("wrapper",Object(r.a)({},c,s,{components:t,mdxType:"MDXLayout"}),Object(i.b)("h1",{id:"what-is-aws-dms"},"What is AWS DMS?"),Object(i.b)("p",null,"AWS Data Migration Service (DMS) is a service that allows us to migrate data between a source (in our case, on-premises database) and a target (in our case, Postgres database hosted in AWS)."),Object(i.b)("h2",{id:"dms-supported-replication-types"},"DMS supported replication types"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},Object(i.b)("strong",{parentName:"p"},"Continuous replication (CDC)")),Object(i.b)("ul",{parentName:"li"},Object(i.b)("li",{parentName:"ul"},"When we want to do a one-off migration of all data and then continuously capture new inserts, updates and deletes and reflect them in our target database"))),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},Object(i.b)("strong",{parentName:"p"},"One-off data migration")),Object(i.b)("ul",{parentName:"li"},Object(i.b)("li",{parentName:"ul"},"When the goal is to migrate all data from a source, and is expected that changes will not be captured and reflected")))),Object(i.b)("h1",{id:"which-aws-dms-set-up-to-use"},"Which AWS DMS set up to use?"),Object(i.b)("h2",{id:"for-continuous-migration"},"For continuous migration"),Object(i.b)("h3",{id:"cdc"},"CDC"),Object(i.b)("p",null,"CDC is a SQL server feature, available only on Enterprise and Developer editions."),Object(i.b)("p",null,"It allows for changes to be captured (inserts/updates/deletes)."),Object(i.b)("h4",{id:"use-case"},"Use case"),Object(i.b)("p",null,"When the source database does not have primary keys and you want to migrate data continuously."),Object(i.b)("h3",{id:"ms-replication"},"MS Replication"),Object(i.b)("p",null,"MS Replication is a SQL server feature available on all editions."),Object(i.b)("p",null,"It creates a \u201cdistribution\u201d database and every time there is a change, it is captured and stored in the \u201cdistribution\u201d database."),Object(i.b)("p",null,".MS will then read from that database to reflect the changes in the target database."),Object(i.b)("p",null,Object(i.b)("strong",{parentName:"p"},"Note:")," The sql user created must have ",Object(i.b)("strong",{parentName:"p"},"sysadmin")," permissions to set up replication"),Object(i.b)("p",null,Object(i.b)("strong",{parentName:"p"},"Additional notes:")," Configuration on the source database is required (please see below section). Additionally, SQL servers DO NOT come with MS replication features pre-installed, so the server might require a set up."),Object(i.b)("h4",{id:"use-cases"},"Use cases"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"When you want to migrate data continuously")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"When the SQL server is not Enterprise/Developer edition")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"When the source database has tables, which make use of primary keys"))),Object(i.b)("h2",{id:"for-one-off-set-up"},"For one-off set up"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"No database configuration is required")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"The sql user must have at least db_owner permissions")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"The replication runs ones and migrates the data specified")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"There are no subsequent runs of the migration task, unless triggered with other means"))),Object(i.b)("h3",{id:"use-cases-1"},"Use cases"),Object(i.b)("ol",null,Object(i.b)("li",{parentName:"ol"},Object(i.b)("p",{parentName:"li"},"When only a one-off migration is required")),Object(i.b)("li",{parentName:"ol"},Object(i.b)("p",{parentName:"li"},"When the underlying source database is a reporting server and there are no possible ways to capture updates. In this scenario, we need to daily run a one-off migration, after the reporting server was updated with the latest data"))),Object(i.b)("h1",{id:"how-to-set-up-dms"},"How to set up DMS"),Object(i.b)("h2",{id:"database-set-up"},"Database set up"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},Object(i.b)("a",{parentName:"p",href:"https://docs.google.com/document/d/1EaZ-a8ejQwWQ40OGDGobxhTqtxXvtX9Ydk5mTFASUMo/edit"},"DMS with SQL CDC"))),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},Object(i.b)("a",{parentName:"p",href:"https://docs.google.com/document/d/14kNirloRWXCnla08brXiTihCMIm24chygc1lGUjNVbE/edit?usp=sharing"},"DMS with MS Replication")))),Object(i.b)("h2",{id:"aws-dms-set-up-via-terraform"},"AWS DMS set up via Terraform"),Object(i.b)("p",null,"Both DMS and Postgres can be created via Terraform."),Object(i.b)("h3",{id:"dms"},"DMS"),Object(i.b)("p",null,Object(i.b)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/aws-dms-terraform"},"Template repository and example usage")),Object(i.b)("p",null,Object(i.b)("img",{alt:"DMS example usage",src:a(195).default})),Object(i.b)("p",null,Object(i.b)("strong",{parentName:"p"},"Notes:")),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"Follow the example usage, which also demonstrates how to add table mappings (specifying which tables are to be replicated)")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"The source DB server should be specified with IP and not the server name")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"DMS instance should be in the VPC, where the VPN is set up to ensure communication to on-prem is possible")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("u",null,"  Make sure your DMS instance\u2019s subnet group has only private subnets in it! "))),Object(i.b)("h3",{id:"postgres"},"Postgres"),Object(i.b)("p",null,Object(i.b)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/aws-hackney-common-terraform/tree/master/modules/database/postgres"},"Template repository and example usage")),Object(i.b)("p",null,Object(i.b)("img",{alt:"PostgreSQL example usage",src:a(196).default})),Object(i.b)("p",null,Object(i.b)("strong",{parentName:"p"},"Notes:")),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"DMS does not support Postgres version 12, so use version 11 or older."),Object(i.b)("li",{parentName:"ul"},"Always store passwords in parameter store and do not hardcode them"),Object(i.b)("li",{parentName:"ul"},"\u201cMulti_az\u201d should be true for production databases"),Object(i.b)("li",{parentName:"ul"},"\u201csubnet_ids\u201d requires subnets in 2 different AZs. Make sure those are private subnets to ensure that the database is secure."),Object(i.b)("li",{parentName:"ul"},"Currently not terraformed: To enable traffic from DMS to your Postgres instance, ensure you add to the ingress rules of the database\u2019s security group all traffic   from DMS security group.")),Object(i.b)("p",null,Object(i.b)("img",{alt:"AWS console",src:a(197).default})),Object(i.b)("h2",{id:"data-migration-using-a-data-pipeline"},"Data migration using a data pipeline"),Object(i.b)("p",null,"  ",Object(i.b)("strong",{parentName:"p"}," What is a data pipeline? ")),Object(i.b)("p",null,"  A data pipeline is an automated flow that gets data stored in one location (source) and uploads it to a target destination."),Object(i.b)("h3",{id:"data-pipeline---csv-to-postgres"},"Data pipeline - CSV to Postgres"),Object(i.b)("p",null,"  As of 26/06/2020, we have implemented one data pipeline."),Object(i.b)("p",null,"  The pipeline takes data uploaded in an S3 bucket in .csv format and uploads the data into a Postgres database."),Object(i.b)("p",null,"  ",Object(i.b)("img",{alt:"Data pipelines",src:a(198).default})))}p.isMDXComponent=!0}}]);